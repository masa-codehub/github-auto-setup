## 実装完了報告: TASK-CORELOGIC-AI-RULE-INFERENCE [Core][Task] US-001: AIによる区切り・キーマッピングルール推論ロジック

Issue `https://github.com/masa-codehub/github-auto-setup/issues/182` の実装が完了しました。ご確認をお願いします。

### 主な変更点

今回のタスクで作成・変更した主要なファイルは以下の通りです。

-   `webapp/core_logic/github_automation_tool/domain/models.py`: AIによる推論ルールを格納する`AISuggestedRules`モデルを新規作成。
-   `webapp/core_logic/github_automation_tool/adapters/ai_parser.py`: LangChainのChainを構築し、プロンプトテンプレートとAIモデルを利用して区切り・キーマッピングルールを推論するロジックを実装。信頼度評価機能も追加。
-   `webapp/core_logic/github_automation_tool/infrastructure/config.py`: ルール推論用のプロンプトテンプレートを`config.yaml`から読み込むように更新。
-   `config.yaml.sample`: 新しいプロンプトテンプレートのサンプル設定を追記。
-   `webapp/core_logic/tests/adapters/test_ai_parser.py`: `AIParser`のルール推論ロジックと信頼度評価に関する単体テストを追加。

### テストと検証

-   `test_requirements.md` に以下のテスト要件を追記・更新しました。
    -   `TR-AI-Parse-001`: AIパーサーが入力テキストからIssue区切りルール（先頭キー）を正しく推論できる。
    -   `TR-AI-Parse-002`: AIパーサーが入力テキストからキーマッピングルールを正しく推論できる。
    -   `TR-AI-Parse-003`: AIパーサーが推論結果の信頼度を評価し、信頼度が低い場合に警告情報を出力できる。
    -   `TR-AI-Error-001`: AI APIの呼び出しが失敗した場合（認証エラー、APIエラー等）に、`AIParser`が適切に例外をハンドリングできる。
-   上記要件を網羅するテストケースを実装し、全てのテストがパスすることを確認済みです。
-   上記に加え、策定された**完了定義（DoD）の全ての項目をクリア**していることを確認済みです。

### 設計上の判断と学習事項

実装にあたり、以下の点を考慮・判断しました。

-   **[その他の主要な設計判断]**
    -   ルール推論のロジックを`AIParser`に集約することで、後続のルールベース処理（分割、マッピング）との責務を明確に分離しました。AIの役割を「ルールを生成すること」に限定し、実際のパース処理は決定論的なコンポーネントが担う構成としました。
    -   信頼度の評価ロジックは、現時点ではAIが生成したルールの一貫性（例：全てのIssueで同じ先頭キーが見つかるか）と、必須フィールド（titleなど）がマッピングルールに含まれるかを基に判定するシンプルな実装としました。
-   **【新規コーディングルールの追加】**
    -   **今回の実装プロセスで得られた知見から、再発防止のため `coding-rules.yml` に以下のルールを追加しました。**
    -   `CR-020: AIを利用する処理は信頼度評価とフォールバック機構を必須とする`

### レビュー依頼

特に以下の点について、重点的にレビューいただけますと幸いです。

-   `AIParser`に実装されたルール推論ロジックと、LangChainの利用方法が適切であるか。
-   ルール推論のために設計したプロンプトテンプレートが、様々な入力パターンに対して頑健であるか。
-   信頼度評価ロジックの判定基準が妥当であるか、また将来的な拡張性があるか。

ご確認のほど、よろしくお願いいたします。

---
TASK_COMPLETED
